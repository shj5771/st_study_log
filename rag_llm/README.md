# RAG & LLM 개발 기록

##  프로젝트 개요
사내 문서 질의응답을 위한 RAG(Retrieval-Augmented Generation) 시스템 구축

## 🛠 기술 스택
- **RAG Framework**: LangChain, Custom RAG Pipeline
- **Vector Database**: ChromaDB, Faiss
- **Embedding Model**: OpenAI Embeddings, HuggingFace Models
- **LLM**: GPT-4, Claude (API 연동)
- **문서 처리**: PyPDF2, python-docx, BeautifulSoup

##  개발 진행 기록

### 1단계: AI 모델 초기 학습 구조 확립
- [x] **문서 수집**: 사내 주요 문서 수집 및 분류 (정책, 매뉴얼, FAQ 등)
- [x] **문서 전처리**: 다양한 형식의 문서를 텍스트로 변환 및 정제
- [x] **학습 데이터 구조화**: 질의응답 학습을 위한 데이터 구조 설계
- [x] **초기 모델 학습**: 기본 RAG 파이프라인 구축 및 초기 학습 수행

### 2단계: 문서 인덱싱 및 검색 시스템
- [x] **문서 청킹**: 효율적 검색을 위한 문서 분할 전략 수립
- [x] **임베딩 생성**: 문서 청크별 벡터 임베딩 생성 및 저장
- [x] **벡터 DB 구축**: ChromaDB를 활용한 벡터 데이터베이스 구축
- [x] **검색 최적화**: 유사도 검색 알고리즘 최적화 및 성능 튜닝

### 3단계: RAG 파이프라인 고도화
- [x] **검색 정확도 향상**: 다양한 검색 전략 실험 및 최적화
- [x] **컨텍스트 관리**: LLM 입력을 위한 컨텍스트 길이 및 품질 최적화
- [x] **답변 생성 개선**: 프롬프트 엔지니어링을 통한 답변 품질 향상
- [x] **응답 속도 최적화**: 캐싱 및 인덱스 최적화로 응답 시간 단축

### 4단계: Agent 모듈과의 통합
- [x] **모듈 인터페이스**: RAG 모듈을 Agent 시스템과 연동하기 위한 API 설계
- [x] **데이터 흐름 최적화**: Agent ↔ RAG 간 효율적 데이터 교환 구조 구축
- [x] **성능 모니터링**: RAG 시스템 성능 측정 및 모니터링 체계 구축
- [x] **오류 처리**: RAG 검색 실패 및 생성 오류에 대한 예외 처리

## 🏗 RAG 시스템 아키텍처

### RAG 파이프라인 구조
```
사내 문서 입력
       ↓
문서 전처리 및 청킹
       ↓
임베딩 생성 및 벡터 DB 저장
       ↓
사용자 질문 입력
       ↓
질문 임베딩 생성
       ↓
유사도 검색 (Top-K)
       ↓
관련 문서 컨텍스트 구성
       ↓
LLM 답변 생성
       ↓
후처리 및 검증
       ↓
최종 답변 반환
```

##  문서 처리 및 최적화

### 지원 문서 형식
- **PDF**: 사내 정책서, 매뉴얼, 보고서
- **Word**: 회의록, 제안서, 계획서
- **Excel**: 데이터 시트, 통계 자료
- **Web**: 사내 위키, 공지사항
- **Text**: 코드 문서, 설정 파일

### 청킹 전략
```python
# 문서 청킹 예시
def chunk_document(text, chunk_size=1000, overlap=200):
    """
    문서를 의미 단위로 분할
    - chunk_size: 청크당 최대 토큰 수
    - overlap: 청크 간 중복 토큰 수
    """
    chunks = []
    # 의미 기반 분할 로직 구현
    return chunks
```

### 임베딩 최적화
- **모델 선택**: 한국어 특화 임베딩 모델 활용
- **차원 최적화**: 검색 성능과 저장 효율성 균형
- **배치 처리**: 대량 문서 임베딩 시 배치 처리로 성능 향상

##  검색 시스템 최적화

### 성능 지표
- **Precision@K**: 상위 K개 결과 중 정확한 문서 비율
- **Recall@K**: 전체 관련 문서 중 상위 K개에 포함된 비율
- **MRR**: Mean Reciprocal Rank (평균 역순위)
- **응답 시간**: 검색부터 답변 생성까지 소요 시간

##  LLM 통합 및 답변 생성

### 프롬프트 엔지니어링(실제와 다름)
```python
SYSTEM_PROMPT = """
당신은 회사 내부 문서를 기반으로 질문에 답하는 AI 어시스턴트입니다.
- 주어진 문서 내용만을 참고하여 답변하세요
- 확실하지 않은 정보는 추측하지 마세요
- 답변의 근거가 되는 문서를 명시하세요
"""

USER_PROMPT = """
문서 내용:
{context}

질문: {question}

답변:
"""
```

### 답변 품질 향상
- **컨텍스트 필터링**: 관련성 높은 문서만 선별하여 입력
- **답변 검증**: 생성된 답변의 사실성 및 적절성 검증
- **출처 표시**: 답변 근거가 되는 문서 정보 제공
- **불확실성 표현**: 확신 수준에 따른 답변 조정

##  성능 모니터링 및 최적화

### 시스템 성능 지표
- **검색 정확도**: 92% (목표: 95%)
- **평균 응답 시간**: 1.8초 (목표: 2초 이내)
- **문서 커버리지**: 87% (인덱싱된 문서 비율)
- **사용자 만족도**: 4.2/5.0 (피드백 기반)

### 지속적 개선
- **피드백 학습**: 사용자 피드백을 통한 검색 결과 개선
- **문서 업데이트**: 신규 문서 자동 인덱싱 및 기존 문서 갱신
- **성능 모니터링**: 실시간 성능 지표 추적 및 알림
- **A/B 테스팅**: 다양한 검색 전략 및 모델 성능 비교

##  주요 성과 및 개선사항

### 기술적 성과
1. **사내 문서 특화**: 일반적인 RAG보다 사내 문서에 최적화된 검색 성능 달성
2. **모듈화 설계**: RAG 구성 요소를 독립 모듈로 분리하여 유지보수성 향상
3. **확장성 확보**: 대용량 문서 처리 및 실시간 업데이트 지원
4. **통합 시스템**: Agent 모듈과의 완전한 통합으로 end-to-end 시스템 구축

